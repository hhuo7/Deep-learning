{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim, nn, unsqueeze\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)), transforms.Grayscale(num_output_channels=1)])\n",
    "\n",
    "trainset0 = datasets.ImageFolder('mnist-varres/train', transform=transform)\n",
    "testset = datasets.ImageFolder('mnist-varres/test', transform=transform)\n",
    "\n",
    "# Divide data in the three resolutions\n",
    "\n",
    "train_res_1 = []\n",
    "train_res_2 = []\n",
    "train_res_3 = []\n",
    "\n",
    "test_res_1 = []\n",
    "test_res_2 = []\n",
    "test_res_3 = []\n",
    "\n",
    "for item in trainset0: \n",
    "    if item[0].shape[1] == 32: train_res_1.append(item)\n",
    "    elif item[0].shape[1] == 48: train_res_2.append(item)\n",
    "    elif item[0].shape[1] == 64: train_res_3.append(item)\n",
    "        \n",
    "for item in testset: \n",
    "    if item[0].shape[1] == 32: test_res_1.append(item)\n",
    "    elif item[0].shape[1] == 48: test_res_2.append(item)\n",
    "    elif item[0].shape[1] == 64: test_res_3.append(item)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Also create a validation set \n",
    "\n",
    "trainset_1, valset_1 = train_test_split(train_res_1, test_size=0.9, random_state=42)\n",
    "trainset_2, valset_2 = train_test_split(train_res_2, test_size=0.9, random_state=42)\n",
    "trainset_3, valset_3 = train_test_split(train_res_3, test_size=0.9, random_state=42)\n",
    "\n",
    "# Create data loaders for each resolution\n",
    "\n",
    "trainloader_1 = torch.utils.data.DataLoader(trainset_1, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "trainloader_2 = torch.utils.data.DataLoader(trainset_2, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "trainloader_3 = torch.utils.data.DataLoader(trainset_3, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "valloader_1 = torch.utils.data.DataLoader(valset_1, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valloader_2 = torch.utils.data.DataLoader(valset_2, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valloader_3 = torch.utils.data.DataLoader(valset_3, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "testloader_1 = torch.utils.data.DataLoader(test_res_1, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader_2 = torch.utils.data.DataLoader(test_res_2, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader_3 = torch.utils.data.DataLoader(test_res_3, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "N = 81\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, mean_pooling):\n",
    "        \n",
    "        self.mean_pooling = mean_pooling\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.conv_neural_network_layers = nn.Sequential(\n",
    "                \n",
    "                # first conv layer\n",
    "                nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1, stride=1),                \n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "                # second conv layer\n",
    "                nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "                # second conv layer\n",
    "                nn.Conv2d(in_channels=32, out_channels=N, kernel_size=3, padding=1, stride=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        #linear layer\n",
    "        self.linear_layers = nn.Sequential(\n",
    "                nn.Linear(N, 10))\n",
    "\n",
    "    # Defining the forward pass \n",
    "    def forward(self, x):\n",
    "        x = self.conv_neural_network_layers(x)\n",
    "\n",
    "        #global max pooling\n",
    "        if self.mean_pooling:\n",
    "            x = torch.flatten(F.adaptive_avg_pool2d(x, (1, 1)), 1)\n",
    "        \n",
    "        else:\n",
    "            x = torch.flatten(F.adaptive_max_pool2d(x, (1, 1)), 1)\n",
    "        \n",
    "        #linear layer\n",
    "        x = self.linear_layers(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        y_hot = F.one_hot(y, 10)\n",
    "        y_hot = torch.zeros(X.shape[0], 10)\n",
    "        y_hot[range(y_hot.shape[0]), y]=1      \n",
    "\n",
    "        X, y_hot = X.to(device), y_hot.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y_hot)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    return loss\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        y_hot = F.one_hot(y, 10)\n",
    "        y_hot = torch.zeros(X.shape[0], 10)\n",
    "        y_hot[range(y_hot.shape[0]), y]=1      \n",
    "\n",
    "        X, y_hot = X.to(device), y_hot.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        test_loss += loss_fn(pred, y_hot).item()\n",
    "        correct += (pred.argmax(axis=1) == y_hot.argmax(axis=1)).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= 10000\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, 100*correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "model_mean_pooling = Network(mean_pooling=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_fn = nn.BCEWithLogitsLoss() #nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model_mean_pooling.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "train_res = [trainloader_1, trainloader_2, trainloader_3]\n",
    "val_res = [valloader_1, valloader_2, valloader_3]\n",
    "test_res = [testloader_1, testloader_2, testloader_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of parameters from N=64 (fixed resolution network) is 29066. \n",
    "#for this network: when N=81, number of parameters is the closest (29029) --> \n",
    "#Thus,N=81 is used in the following experiments.\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"Number of parameters: \", count_parameters(model_mean_pooling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b611c12bf404390108e7f90f5a4d0f5f46de3a8de0eb43725d34779ecc1080a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
